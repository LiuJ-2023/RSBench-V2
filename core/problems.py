"""
Problem definitions for LLM-based Recommender Systems.

This module provides base classes and specific problem implementations
for multi-objective optimization in recommender systems.
"""

import numpy as np
import random
import asyncio
import time
import re
import os
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Tuple, Optional
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatZhipuAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.callbacks.base import AsyncCallbackHandler

from .operators import TokenCounterCallback


class BaseProblem(ABC):
    """
    Abstract base class for all optimization problems.
    
    This class defines the interface that all problem implementations
    must follow, including evaluation methods and data handling.
    """
    
    def __init__(self, 
                 train_data: List[Dict],
                 batch_size: int,
                 api_key: str,
                 llm_model: str = 'gpt',
                 **kwargs):
        """
        Initialize the base problem.
        
        Args:
            train_data: Training data for the problem
            batch_size: Number of samples to use for evaluation
            api_key: API key for LLM services
            llm_model: LLM model to use ('gpt' or 'glm')
            **kwargs: Additional problem-specific parameters
        """
        self.train_data = train_data
        self.batch_size = batch_size
        self.api_key = api_key
        self.llm_model = llm_model
        
        # Token usage tracking
        self.token_stats = {
            'total_tokens': 0,
            'prompt_tokens': 0,
            'completion_tokens': 0,
            'total_cost': 0.0
        }
        
        # Initialize LLM components
        self._setup_llm_components()
        
        # Sample data for evaluation
        self.sample_data = None
    
    @abstractmethod
    def _setup_llm_components(self):
        """Setup LLM components specific to the problem."""
        pass
    
    @abstractmethod
    def sample_test_data(self):
        """Sample test data for evaluation."""
        pass
    
    @abstractmethod
    async def evaluate(self, population: List[str]) -> np.ndarray:
        """
        Evaluate a population of prompts.
        
        Args:
            population: List of prompt strings to evaluate
            
        Returns:
            numpy array of objective values for each individual
        """
        pass
    
    def get_objective_count(self) -> int:
        """Return the number of objectives for this problem."""
        return getattr(self, 'obj_num', 2)


class AccDivProblem(BaseProblem):
    """
    Accuracy vs Diversity problem for recommender systems.
    
    This problem optimizes two objectives:
    1. Accuracy: How well the recommendations match user preferences
    2. Diversity: How diverse the recommended items are
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.obj_num = 2
    
    def _setup_llm_components(self):
        """Setup LLM components for accuracy-diversity problem."""
        # Initialize recommendation LLM
        if self.llm_model == 'glm':
            os.environ["ZHIPUAI_API_KEY"] = self.api_key
            self.llm_recommend = ChatZhipuAI(model="glm-4")
        elif self.llm_model == 'gpt':
            self.llm_recommend = ChatOpenAI(api_key=self.api_key)
        
        self.output_parser = StrOutputParser()
        
        # Setup translation LLM
        self._setup_translation_llm()
        
        # Setup correction LLM
        self._setup_correction_llm()
    
    def _setup_translation_llm(self):
        """Setup LLM for translating responses to lists."""
        prompt_translate = ChatPromptTemplate.from_messages([
            ("user", """Please transfer a set of product names into a list that can be read by python.
              Please note that:
              1. All of the elements in your generated list should be enclosed by quotes.
              2. The name of the output list should be named as "output"
              3. The set of product names is given as follows: {input}""")
        ])
        
        if self.llm_model == 'glm':
            llm_translate = ChatZhipuAI(model="glm-4")
        elif self.llm_model == 'gpt':
            llm_translate = ChatOpenAI(api_key=self.api_key)
        
        self.chain_translate = prompt_translate | llm_translate | StrOutputParser()
    
    def _setup_correction_llm(self):
        """Setup LLM for correcting malformed responses."""
        prompt_correct = ChatPromptTemplate.from_messages([
            ("user", """The python list generated by you cannot be executed by python directly.
             I will give you the python list you just generate. Please check the error and regenerate it.
             A possible reason may be the wrong using of quotation marks.
             The python list you just generated is given as follows: {input}""")
        ])
        
        if self.llm_model == 'glm':
            llm_correct = ChatZhipuAI(model="glm-4")
        elif self.llm_model == 'gpt':
            llm_correct = ChatOpenAI(api_key=self.api_key)
        
        self.chain_correct = prompt_correct | llm_correct | StrOutputParser()
    
    def sample_test_data(self):
        """Sample test data for evaluation."""
        self.sample_data = random.sample(self.train_data, self.batch_size)
    
    async def evaluate(self, population: List[str]) -> np.ndarray:
        """
        Evaluate population for accuracy and diversity objectives.
        Optimized for maximum parallelism - ALL prompt-sample combinations run in parallel.
        
        Args:
            population: List of prompt strings
            
        Returns:
            numpy array of shape (len(population), 2) with [accuracy, diversity] values
        """
        if self.sample_data is None:
            self.sample_test_data()
        
        print(f'ðŸš€ TRULY ASYNC EVALUATION')
        print(f'âš¡ ALL prompts and ALL samples run in parallel with asyncio!')
        print(f'ðŸ”§ Processing {len(population)} prompts with {len(self.sample_data)} samples each')
        
        start_total = time.time()
        
        # Create all LLM tasks for all prompt-sample combinations
        all_tasks = []
        task_info = []  # Track which task belongs to which prompt
        
        print(f'ðŸš€ Creating {len(population) * len(self.sample_data)} async LLM tasks...')
        
        for prompt_idx, prompt in enumerate(population):
            # Create prompt chain for this prompt
            prompt_template = ChatPromptTemplate.from_messages([
                ("system", "You are a recommender for shopping"),
                ("user", f"{prompt}\nNote that, you should make the recommendation for only the candidate set\nThe samples are listed as follows:\n{{samples}}")
            ])
            chain = prompt_template | self.llm_recommend | self.output_parser
            
            # Create async task for each sample of this prompt
            for sample_idx, data in enumerate(self.sample_data):
                task = self._evaluate_single_sample_async(chain, data, prompt_idx, sample_idx)
                all_tasks.append(task)
                task_info.append((prompt_idx, sample_idx))
        
        print(f'âš¡ Executing {len(all_tasks)} LLM calls in parallel...')
        
        # Execute ALL tasks in parallel with controlled concurrency
        max_concurrent = 20  # Higher concurrency for better performance
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def task_with_semaphore(task, task_idx):
            async with semaphore:
                return await task
        
        # Run all tasks with semaphore control
        semaphore_tasks = [task_with_semaphore(task, i) for i, task in enumerate(all_tasks)]
        results = await asyncio.gather(*semaphore_tasks, return_exceptions=True)
        
        end_total = time.time()
        total_time = end_total - start_total
        
        # Organize results by prompt
        prompt_results = {}
        for i, result in enumerate(results):
            prompt_idx, sample_idx = task_info[i]
            if prompt_idx not in prompt_results:
                prompt_results[prompt_idx] = []
            
            if isinstance(result, Exception):
                print(f'âŒ Sample {sample_idx+1} for prompt {prompt_idx+1} failed: {str(result)}')
                prompt_results[prompt_idx].append([1.0, 1.0])
            else:
                prompt_results[prompt_idx].append(result)
        
        # Calculate final objectives for each prompt
        objectives = []
        for prompt_idx in range(len(population)):
            if prompt_idx in prompt_results:
                sample_results = prompt_results[prompt_idx]
                # Average across samples
                avg_result = np.mean(sample_results, axis=0)
                objectives.append(avg_result.tolist())
            else:
                objectives.append([1.0, 1.0])
        
        print(f'ðŸ“Š Evaluation Summary: {len(population)} prompts, {len(all_tasks)} samples, {total_time:.2f}s total')
        
        return np.array(objectives)
    
    async def _evaluate_single_sample_async(self, chain, data, prompt_idx, sample_idx):
        """Async evaluation of a single sample - optimized for pure asyncio"""
        try:
            # Create token counter
            token_counter = TokenCounterCallback(self.token_stats, self.llm_model)
            
            # Single LLM call with timeout
            response = await asyncio.wait_for(
                chain.ainvoke({"samples": data["input"]}, config={"callbacks": [token_counter]}),
                timeout=30.0
            )
            
            # Fast async translation
            response_list = await self._async_translate_fast(response)
            
            # Quick evaluation
            accuracy = self._calculate_accuracy(response_list, data)
            diversity = self._calculate_diversity(response_list, data)
            
            return [accuracy, diversity]
            
        except Exception as e:
            print(f'âŒ Sample {sample_idx+1} for prompt {prompt_idx+1} failed: {str(e)}')
            return [1.0, 1.0]
    
    async def _async_translate_fast(self, input_text):
        """Fast async translation method for pure asyncio approach"""
        try:
            # Create token counter
            token_counter = TokenCounterCallback(self.token_stats, self.llm_model)
            
            # Async translation call with timeout
            response = await asyncio.wait_for(
                self.chain_translate.ainvoke(
                    {"input": input_text}, 
                    config={"callbacks": [token_counter]}
                ),
                timeout=15.0
            )
            
            # Parse response based on model type
            if self.llm_model == 'glm':
                response = re.findall(r"```python\s(.*?)```", response, re.DOTALL)[0]
            
            # Execute the parsed response
            record = {}
            exec(response, {}, record)  # Use empty globals to avoid pollution
            return record["output"]
            
        except Exception as e:
            print(f'âŒ Fast async translation failed: {str(e)}')
            return []
    
    async def _evaluate_single_prompt(self, prompt: str) -> List[float]:
        """Legacy method - redirects to async version."""
        return await self._evaluate_single_prompt_async(prompt)
    
    async def _translate_response_async(self, response: str) -> List[str]:
        """Fast async translation method."""
        try:
            token_counter = TokenCounterCallback(self.token_stats, self.llm_model)
            translated = await asyncio.wait_for(
                self.chain_translate.ainvoke(
                    {"input": response}, 
                    config={"callbacks": [token_counter]}
                ),
                timeout=15.0  # 15 second timeout for translation
            )
            
            # Parse the response
            if self.llm_model == 'glm':
                translated = re.findall(r"```python\s(.*?)```", translated, re.DOTALL)[0]
            
            # Execute to get the list
            record = {}
            exec(translated, {}, record)
            return record.get("output", [])
            
        except asyncio.TimeoutError:
            print(f"Translation timed out")
            return []
        except Exception as e:
            print(f"Fast async translation failed: {e}")
            return []
    
    async def _translate_response(self, response: str) -> List[str]:
        """Legacy method - redirects to async version."""
        return await self._translate_response_async(response)
    
    def _calculate_accuracy(self, recommendations: List[str], data: Dict) -> float:
        """Calculate accuracy based on target item position."""
        try:
            target = data['target']
            if target in recommendations:
                position = recommendations.index(target)
                return position / (len(recommendations) + 1e-10)
            else:
                return 1.0  # Target not found, maximum penalty
        except:
            return 1.0
    
    def _calculate_diversity(self, recommendations: List[str], data: Dict) -> float:
        """Calculate diversity based on item categories."""
        try:
            categories = []
            for item in recommendations[:10]:  # Top 10 recommendations
                try:
                    idx = data["candidate_set"].index(item)
                    categories.extend(data["category_list"][idx])
                except:
                    pass
            
            unique_categories = len(set(categories))
            total_categories = len(categories) + 1e-10
            return 1 - (unique_categories / total_categories)  # Lower is better
        except:
            return 1.0


class AccFairProblem(BaseProblem):
    """
    Accuracy vs Fairness problem for recommender systems.
    
    This problem optimizes two objectives:
    1. Accuracy: How well the recommendations match user preferences
    2. Fairness: How fair the recommendations are across different groups
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.obj_num = 2
    
    def _setup_llm_components(self):
        """Setup LLM components for accuracy-fairness problem."""
        # Similar to AccDivProblem but with fairness calculation
        if self.llm_model == 'glm':
            os.environ["ZHIPUAI_API_KEY"] = self.api_key
            self.llm_recommend = ChatZhipuAI(model="glm-4")
        elif self.llm_model == 'gpt':
            self.llm_recommend = ChatOpenAI(api_key=self.api_key)
        
        self.output_parser = StrOutputParser()
        self._setup_translation_llm()
        self._setup_correction_llm()
    
    def _setup_translation_llm(self):
        """Setup LLM for translating responses to lists."""
        prompt_translate = ChatPromptTemplate.from_messages([
            ("user", """Please transfer a set of product names into a list that can be read by python.
              Please note that:
              1. All of the elements in your generated list should be enclosed by quotes.
              2. The name of the output list should be named as "output"
              3. The set of product names is given as follows: {input}""")
        ])
        
        if self.llm_model == 'glm':
            llm_translate = ChatZhipuAI(model="glm-4")
        elif self.llm_model == 'gpt':
            llm_translate = ChatOpenAI(api_key=self.api_key)
        
        self.chain_translate = prompt_translate | llm_translate | StrOutputParser()
    
    def _setup_correction_llm(self):
        """Setup LLM for correcting malformed responses."""
        prompt_correct = ChatPromptTemplate.from_messages([
            ("user", """The python list generated by you cannot be executed by python directly.
             I will give you the python list you just generate. Please check the error and regenerate it.
             A possible reason may be the wrong using of quotation marks.
             The python list you just generated is given as follows: {input}""")
        ])
        
        if self.llm_model == 'glm':
            llm_correct = ChatZhipuAI(model="glm-4")
        elif self.llm_model == 'gpt':
            llm_correct = ChatOpenAI(api_key=self.api_key)
        
        self.chain_correct = prompt_correct | llm_correct | StrOutputParser()
    
    def sample_test_data(self):
        """Sample test data for evaluation."""
        self.sample_data = random.sample(self.train_data, self.batch_size)
    
    async def evaluate(self, population: List[str]) -> np.ndarray:
        """Evaluate population for accuracy and fairness objectives."""
        if self.sample_data is None:
            self.sample_test_data()
        
        print(f'ðŸš€ TRULY ASYNC EVALUATION')
        print(f'âš¡ ALL prompts and ALL samples run in parallel with asyncio!')
        print(f'ðŸ”§ Processing {len(population)} prompts with {len(self.sample_data)} samples each')
        
        start_total = time.time()
        
        # Create all LLM tasks for all prompt-sample combinations
        all_tasks = []
        task_info = []  # Track which task belongs to which prompt
        
        print(f'ðŸš€ Creating {len(population) * len(self.sample_data)} async LLM tasks...')
        
        for prompt_idx, prompt in enumerate(population):
            # Create prompt chain for this prompt
            prompt_template = ChatPromptTemplate.from_messages([
                ("system", "You are a recommender for shopping"),
                ("user", f"{prompt}\nNote that, you should make the recommendation for only the candidate set\nThe samples are listed as follows:\n{{samples}}")
            ])
            chain = prompt_template | self.llm_recommend | self.output_parser
            
            # Create async task for each sample of this prompt
            for sample_idx, data in enumerate(self.sample_data):
                task = self._evaluate_single_sample_async(chain, data, prompt_idx, sample_idx)
                all_tasks.append(task)
                task_info.append((prompt_idx, sample_idx))
        
        print(f'âš¡ Executing {len(all_tasks)} LLM calls in parallel...')
        
        # Execute ALL tasks in parallel with controlled concurrency
        max_concurrent = 20  # Higher concurrency for better performance
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def task_with_semaphore(task, task_idx):
            async with semaphore:
                return await task
        
        # Run all tasks with semaphore control
        semaphore_tasks = [task_with_semaphore(task, i) for i, task in enumerate(all_tasks)]
        results = await asyncio.gather(*semaphore_tasks, return_exceptions=True)
        
        end_total = time.time()
        total_time = end_total - start_total
        
        # Organize results by prompt
        prompt_results = {}
        for i, result in enumerate(results):
            prompt_idx, sample_idx = task_info[i]
            if prompt_idx not in prompt_results:
                prompt_results[prompt_idx] = []
            
            if isinstance(result, Exception):
                print(f'âŒ Sample {sample_idx+1} for prompt {prompt_idx+1} failed: {str(result)}')
                prompt_results[prompt_idx].append([1.0, 1.0])
            else:
                prompt_results[prompt_idx].append(result)
        
        # Calculate final objectives for each prompt
        objectives = []
        for prompt_idx in range(len(population)):
            if prompt_idx in prompt_results:
                sample_results = prompt_results[prompt_idx]
                # Average across samples
                avg_result = np.mean(sample_results, axis=0)
                objectives.append(avg_result.tolist())
            else:
                objectives.append([1.0, 1.0])
        
        print(f'ðŸ“Š Evaluation Summary: {len(population)} prompts, {len(all_tasks)} samples, {total_time:.2f}s total')
        
        return np.array(objectives)
    
    async def _evaluate_single_sample_async(self, chain, data, prompt_idx, sample_idx):
        """Async evaluation of a single sample - optimized for pure asyncio"""
        try:
            # Create token counter
            token_counter = TokenCounterCallback(self.token_stats, self.llm_model)
            
            # Single LLM call with timeout
            response = await asyncio.wait_for(
                chain.ainvoke({"samples": data["input"]}, config={"callbacks": [token_counter]}),
                timeout=30.0
            )
            
            # Fast async translation
            response_list = await self._async_translate_fast(response)
            
            # Quick evaluation
            accuracy = self._calculate_accuracy(response_list, data)
            fairness = self._calculate_fairness(response_list, data)
            
            return [accuracy, fairness]
            
        except Exception as e:
            print(f'âŒ Sample {sample_idx+1} for prompt {prompt_idx+1} failed: {str(e)}')
            return [1.0, 1.0]
    
    async def _async_translate_fast(self, input_text):
        """Fast async translation method for pure asyncio approach"""
        try:
            # Create token counter
            token_counter = TokenCounterCallback(self.token_stats, self.llm_model)
            
            # Async translation call with timeout
            response = await asyncio.wait_for(
                self.chain_translate.ainvoke(
                    {"input": input_text}, 
                    config={"callbacks": [token_counter]}
                ),
                timeout=15.0
            )
            
            # Parse response based on model type
            if self.llm_model == 'glm':
                response = re.findall(r"```python\s(.*?)```", response, re.DOTALL)[0]
            
            # Execute the parsed response
            record = {}
            exec(response, {}, record)  # Use empty globals to avoid pollution
            return record["output"]
            
        except Exception as e:
            print(f'âŒ Fast async translation failed: {str(e)}')
            return []
    
    async def _evaluate_single_prompt(self, prompt: str) -> List[float]:
        """Legacy method - redirects to async version."""
        return await self._evaluate_single_prompt_async(prompt)
    
    async def _translate_response_async(self, response: str) -> List[str]:
        """Fast async translation method."""
        try:
            token_counter = TokenCounterCallback(self.token_stats, self.llm_model)
            translated = await asyncio.wait_for(
                self.chain_translate.ainvoke(
                    {"input": response}, 
                    config={"callbacks": [token_counter]}
                ),
                timeout=15.0  # 15 second timeout for translation
            )
            
            # Parse the response
            if self.llm_model == 'glm':
                translated = re.findall(r"```python\s(.*?)```", translated, re.DOTALL)[0]
            
            # Execute to get the list
            record = {}
            exec(translated, {}, record)
            return record.get("output", [])
            
        except asyncio.TimeoutError:
            print(f"Translation timed out")
            return []
        except Exception as e:
            print(f"Fast async translation failed: {e}")
            return []
    
    async def _translate_response(self, response: str) -> List[str]:
        """Legacy method - redirects to async version."""
        return await self._translate_response_async(response)
    
    def _calculate_accuracy(self, recommendations: List[str], data: Dict) -> float:
        """Calculate accuracy based on target item position."""
        try:
            target = data['target']
            if target in recommendations:
                position = recommendations.index(target)
                return position / (len(recommendations) + 1e-10)
            else:
                return 1.0
        except:
            return 1.0
    
    def _calculate_fairness(self, recommendations: List[str], data: Dict) -> float:
        """Calculate fairness based on popularity distribution."""
        try:
            popularity_scores = []
            for item in recommendations[:10]:
                try:
                    idx = data["candidate_set"].index(item)
                    popularity_scores.append(data["popular_list"][idx])
                except:
                    popularity_scores.append(0)
            
            # Calculate average popularity (APT - Average Popularity of Top items)
            avg_popularity = np.mean(popularity_scores) if popularity_scores else 0
            return 1 - avg_popularity  # Lower average popularity = higher fairness
        except:
            return 1.0


class AccDivFairProblem(BaseProblem):
    """
    Accuracy vs Diversity vs Fairness problem for recommender systems.
    
    This problem optimizes three objectives:
    1. Accuracy: How well the recommendations match user preferences
    2. Diversity: How diverse the recommended items are
    3. Fairness: How fair the recommendations are across different groups
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.obj_num = 3
    
    def _setup_llm_components(self):
        """Setup LLM components for three-objective problem."""
        if self.llm_model == 'glm':
            os.environ["ZHIPUAI_API_KEY"] = self.api_key
            self.llm_recommend = ChatZhipuAI(model="glm-4")
        elif self.llm_model == 'gpt':
            self.llm_recommend = ChatOpenAI(api_key=self.api_key)
        
        self.output_parser = StrOutputParser()
        self._setup_translation_llm()
        self._setup_correction_llm()
    
    def _setup_translation_llm(self):
        """Setup LLM for translating responses to lists."""
        prompt_translate = ChatPromptTemplate.from_messages([
            ("user", """Please transfer a set of product names into a list that can be read by python.
              Please note that:
              1. All of the elements in your generated list should be enclosed by quotes.
              2. The name of the output list should be named as "output"
              3. The set of product names is given as follows: {input}""")
        ])
        
        if self.llm_model == 'glm':
            llm_translate = ChatZhipuAI(model="glm-4")
        elif self.llm_model == 'gpt':
            llm_translate = ChatOpenAI(api_key=self.api_key)
        
        self.chain_translate = prompt_translate | llm_translate | StrOutputParser()
    
    def _setup_correction_llm(self):
        """Setup LLM for correcting malformed responses."""
        prompt_correct = ChatPromptTemplate.from_messages([
            ("user", """The python list generated by you cannot be executed by python directly.
             I will give you the python list you just generate. Please check the error and regenerate it.
             A possible reason may be the wrong using of quotation marks.
             The python list you just generated is given as follows: {input}""")
        ])
        
        if self.llm_model == 'glm':
            llm_correct = ChatZhipuAI(model="glm-4")
        elif self.llm_model == 'gpt':
            llm_correct = ChatOpenAI(api_key=self.api_key)
        
        self.chain_correct = prompt_correct | llm_correct | StrOutputParser()
    
    def sample_test_data(self):
        """Sample test data for evaluation."""
        self.sample_data = random.sample(self.train_data, self.batch_size)
    
    async def evaluate(self, population: List[str]) -> np.ndarray:
        """Evaluate population for accuracy, diversity, and fairness objectives."""
        if self.sample_data is None:
            self.sample_test_data()
        
        print(f'ðŸš€ TRULY ASYNC EVALUATION')
        print(f'âš¡ ALL prompts and ALL samples run in parallel with asyncio!')
        print(f'ðŸ”§ Processing {len(population)} prompts with {len(self.sample_data)} samples each')
        
        start_total = time.time()
        
        # Create all LLM tasks for all prompt-sample combinations
        all_tasks = []
        task_info = []  # Track which task belongs to which prompt
        
        print(f'ðŸš€ Creating {len(population) * len(self.sample_data)} async LLM tasks...')
        
        for prompt_idx, prompt in enumerate(population):
            # Create prompt chain for this prompt
            prompt_template = ChatPromptTemplate.from_messages([
                ("system", "You are a recommender for shopping"),
                ("user", f"{prompt}\nNote that, you should make the recommendation for only the candidate set\nThe samples are listed as follows:\n{{samples}}")
            ])
            chain = prompt_template | self.llm_recommend | self.output_parser
            
            # Create async task for each sample of this prompt
            for sample_idx, data in enumerate(self.sample_data):
                task = self._evaluate_single_sample_async(chain, data, prompt_idx, sample_idx)
                all_tasks.append(task)
                task_info.append((prompt_idx, sample_idx))
        
        print(f'âš¡ Executing {len(all_tasks)} LLM calls in parallel...')
        
        # Execute ALL tasks in parallel with controlled concurrency
        max_concurrent = 20  # Higher concurrency for better performance
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def task_with_semaphore(task, task_idx):
            async with semaphore:
                return await task
        
        # Run all tasks with semaphore control
        semaphore_tasks = [task_with_semaphore(task, i) for i, task in enumerate(all_tasks)]
        results = await asyncio.gather(*semaphore_tasks, return_exceptions=True)
        
        end_total = time.time()
        total_time = end_total - start_total
        
        # Organize results by prompt
        prompt_results = {}
        for i, result in enumerate(results):
            prompt_idx, sample_idx = task_info[i]
            if prompt_idx not in prompt_results:
                prompt_results[prompt_idx] = []
            
            if isinstance(result, Exception):
                print(f'âŒ Sample {sample_idx+1} for prompt {prompt_idx+1} failed: {str(result)}')
                prompt_results[prompt_idx].append([1.0, 1.0, 1.0])
            else:
                prompt_results[prompt_idx].append(result)
        
        # Calculate final objectives for each prompt
        objectives = []
        for prompt_idx in range(len(population)):
            if prompt_idx in prompt_results:
                sample_results = prompt_results[prompt_idx]
                # Average across samples
                avg_result = np.mean(sample_results, axis=0)
                objectives.append(avg_result.tolist())
            else:
                objectives.append([1.0, 1.0, 1.0])
        
        print(f'ðŸ“Š Evaluation Summary: {len(population)} prompts, {len(all_tasks)} samples, {total_time:.2f}s total')
        
        return np.array(objectives)
    
    async def _evaluate_single_sample_async(self, chain, data, prompt_idx, sample_idx):
        """Async evaluation of a single sample - optimized for pure asyncio"""
        try:
            # Create token counter
            token_counter = TokenCounterCallback(self.token_stats, self.llm_model)
            
            # Single LLM call with timeout
            response = await asyncio.wait_for(
                chain.ainvoke({"samples": data["input"]}, config={"callbacks": [token_counter]}),
                timeout=30.0
            )
            
            # Fast async translation
            response_list = await self._async_translate_fast(response)
            
            # Quick evaluation
            accuracy = self._calculate_accuracy(response_list, data)
            diversity = self._calculate_diversity(response_list, data)
            fairness = self._calculate_fairness(response_list, data)
            
            return [accuracy, diversity, fairness]
            
        except Exception as e:
            print(f'âŒ Sample {sample_idx+1} for prompt {prompt_idx+1} failed: {str(e)}')
            return [1.0, 1.0, 1.0]
    
    async def _async_translate_fast(self, input_text):
        """Fast async translation method for pure asyncio approach"""
        try:
            # Create token counter
            token_counter = TokenCounterCallback(self.token_stats, self.llm_model)
            
            # Async translation call with timeout
            response = await asyncio.wait_for(
                self.chain_translate.ainvoke(
                    {"input": input_text}, 
                    config={"callbacks": [token_counter]}
                ),
                timeout=15.0
            )
            
            # Parse response based on model type
            if self.llm_model == 'glm':
                response = re.findall(r"```python\s(.*?)```", response, re.DOTALL)[0]
            
            # Execute the parsed response
            record = {}
            exec(response, {}, record)  # Use empty globals to avoid pollution
            return record["output"]
            
        except Exception as e:
            print(f'âŒ Fast async translation failed: {str(e)}')
            return []
    
    async def _evaluate_single_prompt(self, prompt: str) -> List[float]:
        """Legacy method - redirects to async version."""
        return await self._evaluate_single_prompt_async(prompt)
    
    async def _translate_response_async(self, response: str) -> List[str]:
        """Fast async translation method."""
        try:
            token_counter = TokenCounterCallback(self.token_stats, self.llm_model)
            translated = await asyncio.wait_for(
                self.chain_translate.ainvoke(
                    {"input": response}, 
                    config={"callbacks": [token_counter]}
                ),
                timeout=15.0  # 15 second timeout for translation
            )
            
            # Parse the response
            if self.llm_model == 'glm':
                translated = re.findall(r"```python\s(.*?)```", translated, re.DOTALL)[0]
            
            # Execute to get the list
            record = {}
            exec(translated, {}, record)
            return record.get("output", [])
            
        except asyncio.TimeoutError:
            print(f"Translation timed out")
            return []
        except Exception as e:
            print(f"Fast async translation failed: {e}")
            return []
    
    async def _translate_response(self, response: str) -> List[str]:
        """Legacy method - redirects to async version."""
        return await self._translate_response_async(response)
    
    def _calculate_accuracy(self, recommendations: List[str], data: Dict) -> float:
        """Calculate accuracy based on target item position."""
        try:
            target = data['target']
            if target in recommendations:
                position = recommendations.index(target)
                return position / (len(recommendations) + 1e-10)
            else:
                return 1.0
        except:
            return 1.0
    
    def _calculate_diversity(self, recommendations: List[str], data: Dict) -> float:
        """Calculate diversity based on item categories."""
        try:
            categories = []
            for item in recommendations[:10]:
                try:
                    idx = data["candidate_set"].index(item)
                    categories.extend(data["category_list"][idx])
                except:
                    pass
            
            unique_categories = len(set(categories))
            total_categories = len(categories) + 1e-10
            return 1 - (unique_categories / total_categories)
        except:
            return 1.0
    
    def _calculate_fairness(self, recommendations: List[str], data: Dict) -> float:
        """Calculate fairness based on popularity distribution."""
        try:
            popularity_scores = []
            for item in recommendations[:10]:
                try:
                    idx = data["candidate_set"].index(item)
                    popularity_scores.append(data["popular_list"][idx])
                except:
                    popularity_scores.append(0)
            
            avg_popularity = np.mean(popularity_scores) if popularity_scores else 0
            return 1 - avg_popularity
        except:
            return 1.0
